搜索引擎最重要的构建模块之一就是能够有效地压缩和快速解码排序的整数列表。

# 词项-文档关联矩阵(Incidence matrix)

词项-文档关联矩阵是表示词项和文档之间所具有的一种包含关系的概念模型。表 1 的每列代表一个文档，每行代表一个词项，打对勾的位置代表包含关系。

​																					**表 1 词项-文档关联矩阵**

|       | doc1 | doc2 | doc3 | doc4 | doc5 |
| ----- | ---- | ---- | ---- | ---- | ---- |
| term1 | ✅    |      | ✅    | ✅    |      |
| term2 |      | ✅    | ✅    |      | ✅    |
| term3 |      |      | ✅    |      |      |
| term4 | ✅    |      | ✅    |      | ✅    |
| term5 | ✅    |      |      | ✅    |      |

从纵向即文档这个维度来看，每列代表一个文档包含的词项信息；从横向即词项这个维度来看，每行代表一个词项在文档中的分布信息。

**搜索引擎的索引其实就是实现单词—文档矩阵的具体数据结构**。可以有不同的方式来实现上述概念模型，比如倒排索引、签名文件、后缀树等方式。但是各项实验数据表明，**倒排索引**是单词到文档映射关系的最佳实现方式。



# 倒排索引

## 倒排索引基本概念

- **文档(Document)**

  文档是信息检索的对象，不仅指文本，也可以是图像、视频、语音等多媒体文档。

- **文档集(Crops)**

  由若干文档构成的集合成为文档集合。

- **文档编号(Document ID)** 

  文档 ID 是给文档集中的每一个文档赋予的唯一标识符。 

- **词项(Term)**

  词项是经过语言学预处理之后归一化的词条，词项是索引的最小单位。

- **词项频率(Term frequency)**

  同一个单词在某个文档中出现的频率。

- **文档频率(Document frequency)**

  出现某词项的文档的数目。

- **倒排记录表(Postings lists)**

  倒排记录表用于记录出现过某个单词的所有文档的文档列表以及单词在该文档中出现的位置信息，每条记录称为一个倒排项。通过倒排列表即可获知哪些文档包含哪些单词。

- **倒排文件(Inverted File)**

  倒排记录表在磁盘中的物理存储文件称为倒排文件。

关于这些概念之间的关系，通过图 2 可以比较清晰地看出来：

![image-20190607211606081](http://image.yoyadoc.com/blog/2019-06-11-143221.png)

​																					**图 2 倒排索引基本概念示意图**



## 原理

倒排索引从逻辑结构和基本思路上讲非常简单。下面我们通过具体实例来进行说明：

假设文档集合包含 3 个文档，每个文档内容如表 3 所示，图中最左端一栏是每个文档对应的文档编号，我们的任务就是对这个文档集合建立倒排索引。

| 文档编号 | 文档内容                                                     |
| -------- | ------------------------------------------------------------ |
| 1        | Elasticsearch is a distributed, RESTful search and analytics engine. |
| 2        | Elasticsearch is the Heart of the Elastic Stack.             |
| 3        | Elasticsearch lets you perform and combine many types of searches. |
| 4        | Elasticsearch is fast. Really, really fast.                  |

中文和英文等语言不同，单词之间没有明确的分隔符号，所以首先要用分词系统将文档自动切分成单词序列，这样每个文档就转换为由单词序列构成的数据流。

分词之后，我们可以得到如表 2 所示的最简单的倒排索引。

​																			**表 2 最简单的倒排索引**

| 词项          | 倒排记录表(DocID) |
| ------------- | ----------------- |
| elasticsearch | 1,2,3,4           |
| distribut     | 1                 |
| rest          | 1                 |
| search        | 1,3               |
| analyt        | 1                 |
| engin         | 1                 |
| heart         | 2                 |
| elast         | 2                 |
| stack         | 2                 |
| let           | 3                 |
| you           | 3                 |
| perform       | 3                 |
| combin        | 3                 |
| mani          | 3                 |
| type          | 3                 |
| fast          | 4                 |
| realli        | 4                 |



这个索引系统只记载了哪些文档包含某个单词，而事实上，索引系统还可以记录除此之外的更多信息。

图 3 是一个相对复杂些的倒排索引，还额外记载了词项频率信息(TF)，记这个单词在某个文档中出现的次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是一个很重要的计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。

​																			**图 3 带有词项频率信息的倒排索引**

| 词项          | 倒排记录表(DocID;TF)    |
| ------------- | ----------------------- |
| elasticsearch | (1;1),(2;1),(3;1),(4;1) |
| distribut     | (1;1)                   |
| rest          | (1;1)                   |
| search        | (1;1),(3;1)             |
| analyt        | (1;1)                   |
| engin         | (1;1)                   |
| heart         | (2;1)                   |
| elast         | (2;1)                   |
| stack         | (2;1)                   |
| let           | (3;1)                   |
| you           | (3;1)                   |
| perform       | (3;1)                   |
| combin        | (3;1)                   |
| mani          | (3;1)                   |
| type          | (3;1)                   |
| fast          | (4;2)                   |
| realli        | (4;2)                   |



实用的倒排索引还可以记载更多的信息，如表 4 所示的索引系统除了记录文档编号和单词频率信息外，额外记载了两类信息，即每个单词对应的文档频率信息（对应表 4 的第 2 列）及单词在某个文档出现位置的信息。

​														**图 3 带有单词频率、文档频率和出现位置信息的倒排索引**

| 词项          | 文档频率 | 倒排记录表(DocID;TF;\<POS>)             |
| ------------- | -------- | --------------------------------------- |
| elasticsearch | 4        | (1;1;<0>),(2;1;<0>),(3;1;<0>),(4;1;<0>) |
| distribut     | 1        | (1;1;<3>)                               |
| rest          | 1        | (1;1;<4>)                               |
| search        | 2        | (1;1;<5>),(3;1;<9>)                     |
| analyt        | 1        | (1;1;<7>)                               |
| engin         | 1        | (1;1;<8>)                               |
| heart         | 1        | (2;1;<3>)                               |
| elast         | 1        | (2;1;<6>)                               |
| stack         | 1        | (2;1;<7>)                               |
| let           | 1        | (3;1;<1>)                               |
| you           | 1        | (3;1;<2>)                               |
| perform       | 1        | (3;1;<3>)                               |
| combin        | 1        | (3;1;<5>)                               |
| mani          | 1        | (3;1;<6>)                               |
| type          | 1        | (3;1;<7>)                               |
| fast          | 1        | (4;2;<2,5>)                             |
| realli        | 1        | (4;2;<3,4>)                             |



可以使用以下命令来查看分词效果：

```shell
curl -X POST \
  'http://localhost:9200/_analyze?analyzer=english' \
  -H 'Content-Type: application/json' \
  -d '{
	"text":"Elasticsearch is fast. Really, really fast."
}'
```



# 分词算法

词是表达语义的最小单位。分词对搜索引擎的帮助很大，可以帮助搜索引擎程序自动识别语句的含义，从而使搜索结果的匹配度达到最高，因此分词的质量也就直接影响了搜索结果的精确度。

## 英文分词原理

基本的处理流程是：输入文本、词汇分割、词汇过滤(去掉停留词)、词干提取(形态还原)、大写转为小写、结果输出。

## 中文分词原理

中文分词很难对词的边界进行界定，难以将词或分出来。

中文分词面临三大难题：分词规则、消除歧义和未登录词识别。

- **分词规则**

  构建完美的分词规则便可以将所有的句子正确的划分，但是这根本无法实现，语言是长期发展自然而然形成的，而且语言规则庞大复杂，很难做出完美的分词规则。

- **消除歧义**

  在中文句子中，很多词是由歧义性的，在一句话也可能有多种分词方法。比如：”结婚/的/和尚/未结婚/的“，“结婚/的/和/尚未/结婚/的”，人分辨这样的句子都是问题，更何况是机器。

- **未登录词识别**

  所谓的未登录词是指在分词词典中没有收录，并且确实是大家公认的词语的那些词语，一般又叫做新词。最典型的未登录词就是人名词，“李胜利喜欢唱歌”中“李胜利”是个人名词，如果把“李胜利”这个基本词条收录到字典中去是能解决这个问题。但是，每时每刻都有新增的姓名，完整收录全部人名本身就是一个不现实的工程。中外人名、中国地名、机构组织名、事件名、货币名、缩略语、派生词、各种专业术语以及在不断发展和约定俗成的一些新词语。在当下的互联网时代，人们还会不断的创造出一些新词出来，比如：“神马”、“不明觉厉”等。未登录词辨别未登录词包括是种类繁多，形态组合各异，规模宏大的一个领域。对这些词语的自动辨识，是一件非常困难的事。
  
  新词是中文分词算法在召回层面上最主要的难题，也是评价一个分词系统好坏的重要标志。如果一个新词无法被分词系统识别，会导致很多噪音数据被召回，进而会影响后面的句法分析和语义分析等相关处理。黄昌宁等在中文信息学报上的《中文分词十年回顾》一文指出：新词带来的分词问题是歧义的10倍~20倍，所以说新词发现是分词面临的最大挑战。

中文分词主要有三种方法：基于词典匹配的分词方法、基于语义理解的分词、基于词频统计的分词。

### 词典匹配分词法

基于词典匹配的分词方法按照一定的匹配策略将输入的字符串与机器词典词条进行匹配，这种方法是最简单也是最高效的方法。

查字典分词实际上就是把一个句子从左到右扫描一遍，遇到字典中有的词就标识出来，遇到复合词就找到最长的词匹配，遇到不认识的字串则切分成单个词。

按照匹配操作的扫描方向不同，字典匹配分词法可以分为**正向最大匹配**、**逆向最大匹配**以及结合了两者的**双向最大匹配算法**。

#### 正向最大匹配法(FMM)

FMM 的步骤是：

1. 从左向右取待分词语句的 m 个字作为匹配字段，m 为词典中最长词的长度；
2. 查找词典进行匹配；
3. 若匹配成功，则将该字段作为一个词切分出去；
4. 若匹配不成功，则将该字段最后一个字去掉，剩下的字作为新匹配字段，进行再次匹配；
5. 重复上述过程，直到切分所有词为止。

#### 逆向最大匹配法(RMM)

RMM 的步骤是：

1. 从右向左取待分词语句的 m 个字作为匹配字段，m 为词典中最长词的长度；
2. 查找词典进行匹配；
3. 若匹配成功，则将该字段作为一个词切分出去；
4. 若匹配不成功，则将该字段头部一个字去掉，剩下的字作为新匹配字段，进行再次匹配；
5. 重复上述过程，直到切分所有词为止。

#### 双向最大匹配法(Bi-MM)

Bi-MM 是将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。据 SunM.S. 和 Benjamin K.T.（1995）的研究表明，中文中 90.0% 左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概 9.0% 的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到 1.0% 的句子，使用正向最大匹配法和逆向最大匹配法的切分虽然重合但是错的，或者两种方法切分不同但结果都不对（歧义检测失败）。

Bi-MM 的步骤是：

1. 如果正反向分词结果词数不同，则取分词数量少的那个；

2. 如果分词结果词数相同：

   1）分词结果相同，没有歧义，返回任意一个；

   2）分词结果不同，返回其中单字数量较少的那个。

   

下面举一个例子来说明三种分词的效果：

假设现在的登录词库有：研究生、研究、生命、起源，则三种分词方法效果如下

|       | 研究生命的起源        |
| ----- | --------------------- |
| FMM   | 研究生  命  的  起源  |
| RMM   | 研究  生命  的   起源 |
| Bi-MM | 研究  生命  的   起源 |



### 语义理解分词法

基于语义理解的分词方法是模拟人脑对语言和句子的理解，达到识别词汇单元的效果。基本模式是把分词、句法、语义分析并行运行，利用句法和语义信息来处理分词的歧义。

### 词频统计分词法

这种做法是基于人们对中文词语的直接感觉。通常词是稳定的组合，因此在中文文章的上下文中，相邻的词搭配出现的概率越多，就越有可能形成一个固定的词。根据 n 元语法知识可以知道，字与字相邻同时出现的频率或概率能够较好地反映成词的可信度。

实际的系统中，通过对精心准备的中文语料中相邻共现的各个字的组合的频度进行统计，计算不同字词的共现信息。根据两个字的统计信息，计算两个汉字的相邻共现概率，统计出来的信息体现了中文环境下汉字之间结合的紧密程度。当紧密程度高于某一个阈值时，便可以认为此字组可能构成一个词。

它的优点在于可以发现所有的切分歧义并且容易将新词提取出来。



# 词项词典(Term Dictionary)

假设我们有很多个 term，比如：**Carla,Sara,Elin,Ada,Patty,Kate,Selena**

如果按照这样的顺序排列，找出某个特定的 term 一定很慢，因为 term 没有排序，需要全部过滤一遍才能找出特定的 term。排序之后就变成了：**Ada,Carla,Elin,Kate,Patty,Sara,Selena**

这样我们可以用二分查找的方式，比全遍历更快地找出目标的 term。这个就是 term dictionary。有了 term dictionary 之后，可以用 logN 次磁盘查找得到目标。但是磁盘的随机读操作仍然是非常昂贵的（一次 random access 大概需要 10ms 的时间）。所以尽量少的读磁盘，有必要把一些数据缓存到内存里。但是整个 term dictionary 本身又太大了，无法完整地放到内存里。于是就有了 term index。



词项字典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。

对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。

## 哈希加链表

图 4 是这种词典结构的示意图。这种词典结构主要由两个部分构成，主体部分是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个不同单词获得相同的哈希值，如果是这样，在哈希方法里被称做是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。

![image-20190608201033390](http://image.yoyadoc.com/blog/2019-06-11-143211.png)



## Trie 树

Elasticsearch 为了提高搜索速度，会直接通过内存查找 Term，但是如果 Term 太多，无法将整个 Term dictionary 放进内存，所以需要一种数据结构来满足全内存查找的需求，这种结构就是 Trie 树。

![term-index](http://image.yoyadoc.com/blog/2019-06-11-063315.png)

这棵树不会包含所有的 term，它包含的是 term 的一些前缀。通过 term index 可以快速地定位到 term dictionary 的某个 offset，然后从这个位置再往后顺序查找。

![index](http://image.yoyadoc.com/blog/2019-06-11-063347.png)

所以 term index 不需要存下所有的 term，而仅仅是他们的一些前缀与 Term Dictionary 的 block 之间的映射关系，再结合 FST(Finite State Transducers) 的压缩技术，可以使 term index 缓存到内存中。从 term index 查到对应的 term dictionary 的 block 位置之后，再去磁盘上找 term，大大减少了磁盘随机读的次数。

Term index 在内存中是以 FST（Finite state transducers）的形式保存的，其特点是非常节省内存。Term dictionary 在磁盘上是以分 block 的方式保存的，一个 block 内部利用公共前缀压缩，比如都是 Ab 开头的单词就可以把 Ab 省去。这样 term dictionary 可以更节约磁盘空间。

### FST

Finite state transducers 是一种有限状态转移机。理论依据可查看 [FST论文](https://cs.nyu.edu/~mohri/pub/fla.pdf)。

FST 有两大优点：

- **空间占用小**

  通过对词典中单词前缀和后缀的重复利用，压缩了存储空间

- **查询速度快**

  O(len(str))的查询时间复杂度



[FST Example website](http://examples.mikemccandless.com/fst.py?terms=&cmd=Build+it%21)

![下载](http://image.yoyadoc.com/blog/2019-06-11-063400.png)

⭕️表示一种状态

-->表示状态的变化过程，上面的字母/数字表示状态变化和权重

将单词分成单个字母通过⭕️和-->表示出来，0权重不显示。如果⭕️后面出现分支，就标记权重，最后整条路径上的权重加起来就是这个单词对应的序号。

FST以字节的方式存储所有的 term，这种压缩方式可以有效的缩减存储空间，使得 term index 足以放进内存，但这种方式也会导致查找时需要更多的 CPU 资源。

FST压缩率一般在3倍~20倍之间，相对于TreeMap/HashMap的膨胀3倍，内存节省就有9倍到60倍！但是 FST 的性能略差于 TreeMap 和 HashMap，但是 FST 可以让更多的 Term Index 放进内存，减少的随机磁盘读取的时间远大于这部分的性能损失。

![242314284968965](http://image.yoyadoc.com/blog/2019-06-11-063424.png)



# 倒排记录表(Postings lists)

通过已上分析可以知道，假设搜索条件为 text=elasticsearch, 那么 Elasticsearch 的搜索过程为：

1. 给定查询条件，先从 term index 找到 elasticsearch 在 term dictionary 的大概位置；
2. 从 term dictionary 里精确找到 elasticsearch 这个 term，得到一个 postings list 或者一个指向 postings list 位置的指针；

如果有多个查询条件，那么最终会得到多个 postings list，然后需要对多个 postings list 做交集或并集处理来得到最终结果。

那么如何才能对多个 postings list 做交集处理呢？

下面介绍 Elasticsearch 使用的两种方法：skip list 和 bitset。

Elasticsearch 如果查询的 filter 缓存到了内存中（以 bitset 的形式），那么合并就是两个 bitset 的 AND。如果查询的 filter 没有缓存，那么就用 skip list 的方式去遍历两个 on disk 的 postings list。

## Skip list

![combineIndex](http://image.yoyadoc.com/blog/2019-06-11-063437.png)

以上是三个 postings list。我们现在需要把它们用 AND 的关系合并，得出 postings list 的交集。首先选择最短的 postings list，然后从小到大遍历。

整个过程如下：

```shell
Next -> 2
Advance(2) -> 13
Advance(13) -> 13
Already on 13
Advance(13) -> 13 MATCH!!!
Next -> 17
Advance(17) -> 22
Advance(22) -> 98
Advance(98) -> 98
Advance(98) -> 98 MATCH!!!
```

最后得出的交集是 [13,98]，所需的时间比完整遍历三个 postings list 要快得多。但是前提是每个 list 需要指出 Advance 这个操作，快速移动指向的位置。什么样的 list 可以这样 Advance 往前做蛙跳？

答案就是 **跳表(skip list)**!

从概念上来讲，对于一个很长的 postings list，比如:

```
[12,15,23,31,31,47,55,61,98,127]
```

我们可以把这个 list 分成三个 block：

```
[12,15,23] [31,31,47,55] [61,98,127]
```

然后就可以构建出 skip list 的第二层：

```
[12,31,61]
```

12,31,61 分别指向自己对应的 block，这样就可以很快地跨 block 移动指向位置了。

![skiplist](http://image.yoyadoc.com/blog/2019-06-11-063454.png)

更进一步，Lucence 会对这个 block 使用一种叫做 Frame Of Reference 的编码格式再次进行压缩。

![frameOfReference](http://image.yoyadoc.com/blog/2019-06-11-063508.png)

例如，如果你的 postings list 是`[73, 300, 302, 332, 343, 372]`，则增量列表将是`[73, 227, 2, 30, 11, 29]`。这里有趣的是，所有增量都在 0 到 255 之间，因此每个值只需要一个字节。Lucene 用来对磁盘上的倒排索引进行编码的技术是：

1. 使用 delta 编码；
2. 将编码后的结果拆分为 256 个 doc ID 的 block；
3. 计算储存 block 内数据需要的最小 bit 位数，将此信息添加到 block 头部，然后使用此数量的位编码 block 中的所有增量。



## Bitset

Bitset 是一种很直观的数据结构，它的基本原理是，用一个 bit 位来表示一个数据是否出现过，0 为没有出现过，1 表示出现过。假设 posting list 如下所示：

```
[1,3,4,7,10]
```

那么对应的 bitset 就是：

```
[1,0,1,1,0,0,1,0,0,1]
```

每个文档按照文档 id 排序对应其中的一个 bit。

Bitset 自身就有压缩的特点，其用一个 byte 就可以代表 8 个文档。所以 100 万个文档只需要 12.5 万个 byte。

但是考虑到文档可能有数十亿之多，在内存里保存 bitset 仍然是很奢侈的事情。



在 Lucence 5 中，开始使用 Roaring Bitmap 来代替 Bitset。

![roaring](http://image.yoyadoc.com/blog/2019-06-11-063514.png)

Roaring Bitmap 先将 postings list 拆分为 block，每个 block 大小为 65536，即 2^16。这意味着，第一个 block 将编码 0 ~ 65535 之间的值，第二个 block 将编码 65536 ~ 131071 之间的值。然后再用 **<商，余数>** 的组合表示每一组 DocID，这样每组里的 DocID 范围都在 0 ~ 65535 之间了。最后，使用 2 字节的数组或者 bitset 来保存每一个 block（*如果 block 的大小大于 4096，那么使用 bitset 来保存，否则，使用 2 字节的数组来保存*）。

**为什么是以 65535 来作为阈值？**

因为 65535 = 2^16 - 1，恰好是 2 个字节可以表示的最大数。

**为什么要以 4096 来作为阈值？**

仅仅是因为如果 block 中的文档数量大于 4096，那么使用 bitset 将更节省内存！

![roaring_memory](http://image.yoyadoc.com/blog/2019-06-11-063519.png)



# Elasticsearch 的查询与缓存机制

Elasticsearch 使用 Filter 来过滤非评分查询，假设查询请求如下：

```
POST /my_store/products/_search
{
    "query" : {
        "constant_score" : {
            "filter" : {
                "term" : {
                    "productID" : "XHDK-A-1293-#fJ3"
                }
            }
        }
    }
}
```

Filter 过滤器的操作为：

1. 查找匹配文档

   通过倒排索引查找 `XHDK-A-1293-#fJ3`，然后获得包含该 term 的 postings list。

2. 创建 bitset

   filter 会创建一个 bitset，用于描述哪些文档包含该 term。在内部，这里的 bitset 表示为 Roaring Bitmap。

3. 迭代 bitset(s)

   迭代 bitset 来找到符合要求的文档。这里会先迭代稀疏的 bitset，因为可以先排除掉大量的文档。

Elasticsearch 会基于使用频次自动缓存查询。如果一个非评分查询在最近的 256 次查询中被使用过（次数取决于查询类型），那么这个查询就会作为缓存的候选。但是，并不是所有的片段都能保证缓存 bitset 。只有那些文档数量超过 10,000 （或超过总文档数量的 3% )才会缓存 bitset 。因为小的片段可以很快的进行搜索和合并，这里缓存的意义不大。

一旦缓存了，非评分计算的 bitset 会一直驻留在缓存中直到它被剔除。剔除规则是基于 LRU 的：一旦缓存满了，最近最少使用的过滤器会被剔除。



# TF-IDF 权重计算

tf-idf 称为词频-逆文档频率，用以计算词项对于一个文档集或一个语料库中的一份文件的重要程度。词项的重要性会随着它在文档中出现的次数成正比，但同时会随着它在文档集中出现的频率成反比。

换句话说，如果一个词项在一遍文档中出现的频率非常高，说明其重要性比较高，但是如果这个词项在文档集中出现的频率也很高，那么说明这个词项有可能是比较通用比较常见的。

Elasticsearch 底层是基于 Lucene，而 Lucene采用的评分算法叫做 TF-IDF 算法。

- **TF**    TF(Term Frequency)代表词项频率，即词项在某篇文档中出现的次数。
- **IDF**    IDF(Inverse Document Frequency)代表逆文档频率。



Lucene 打分公式如下：

![image-20190609120732331](http://image.yoyadoc.com/blog/2019-06-11-063526.png)



### coord(q,d)

评分因子，是基于文档中出现查询项的个数。越多的查询项在一个文档中，说明这些文档的匹配程度越高。默认是出现查询项的百分比。

```java
public float coord(int overlap, int maxOverlap) {
    return overlap / (float)maxOverlap;
}
```

- **overlap** 检索命中 query 中 term 的个数
- **maxOverlap** query 中的 term 个数

### queryNorm(q)

**queryNorm(q)** 是一个修正因子（*normalizing factor*），用来使不同查询间的分数可比较(*comparable*)。

这个因素对所有文档都是一样的值，所以它不影响排序结果。比如如果我们希望所有文档的评分大一点，那么我们就需要设置这个值。

```java
public float queryNorm(float sumOfSquaredWeights) {
    return (float)(1.0 / Math.sqrt(sumOfSquaredWeights));
}
```

### tf(t in d)

即 term t 在文档 d 中出现的个数。

```java
public float tf(float freq) {
    return (float)Math.sqrt(freq);
}
```

### idf(t)

文档频率用 df(document frequency) 表示，代表文档集中包含某个词的所有文档数目。df 通常比较大，把它映射到一个较小的取值范围，用逆文档频率 idf(Inverse Document Frequency) 来表示：

```java
public float idf(long docFreq, long numDocs) {
    return (float)(Math.log(numDocs/(double)(docFreq+1)) + 1.0);
}
```

- docFreq 指的是包含某个词项的文档数
- numDocs 指的是文档集的总文档数

### boost(t)

boost 指查询 term 的加权值，这个值可以在建索引时指定，也可以在查询时指定。

### norm(t,d)

这个项是长度的加权因子，目的是为了将同样匹配的文档，比较短的放比较前面。

```
norm(t,d) = doc.getBoost()· lengthNorm· ∏ f.getBoost()
```

这里的doc.getBoost表示文档的权重，f.getBoost表示字段的权重，如果这两个都设置为1，那么norm(t,d)就和lengthNorm一样的值。

```java
public float lengthNorm(FieldInvertState state) {
    final int numTerms;
    if (discountOverlaps)
        numTerms = state.getLength() - state.getNumOverlap();
    else
        numTerms = state.getLength();
    return state.getBoost() * ((float) (1.0 / Math.sqrt(numTerms)));
}
```



根据上述分析，可以得出以下几个规则：

- 匹配到的关键词越稀有，文档的得分就越高；
- 文档的域越小(包含比较少的Term)，文档的得分就越高；
- 设置的权重(索引和搜索时设置的都可以)越大，文档得分越高。



# ElasticSearch Near Real Time 分析

Elasticsearch的核心优势就是（**Near Real Time NRT**）近乎实时，Elasticsearch能够做到准实时，而并不是完全的实时。

## 数据索引

Elasticsearch 索引数据的整个流程如下图所示：

![20190226092548269](http://image.yoyadoc.com/blog/2019-06-11-063532.png)

1. 当一个文档被索引时，它会被添加到 in-memory buffer，并且添加到Translog日志中。

   ![212147_cQz6_2882821](http://image.yoyadoc.com/blog/2019-06-11-063537.png)

2. 每隔一秒，执行一次 refresh 操作，将所有在  in-memory buffer 中的文档写入到一个新的 segment(在内核文件系统) 中，同时清空  in-memory buffer。**这时候数据已经可以被搜索到**。

   ![212212_a6FT_2882821](http://image.yoyadoc.com/blog/2019-06-11-063542.png)

3. 每三十分钟或者当 translog 变得非常大时，索引将会被 flush，内核文件系统会被 fsync 到磁盘， 在磁盘中写入新的 commit point 信息，新的 translog 也将会建立，一个完全的提交进行完毕。

   ![212249_XUiY_2882821](http://image.yoyadoc.com/blog/2019-06-11-063546.png)

translog 可以保证缓存中的 segment 的恢复，但 translog 也不是实时写磁盘的，也就是说，内存中的 translog 丢了的话，也会有丢失数据的可能。所以 translog 也要进行 flush。translog 的 flush 主要有三个条件：

1. 可以设置是否在某些操作之后进行强制 flush，比如索引的删除或批量请求之后。
2. translog 大小超过 512mb 或者超过三十分钟会强制对 segment 进行 flush，随后会强制对 translog 进行 flush，这种情况缓存中的 translog 在 flush 之后会被清空。
3. 默认 5s，会强制对 translog 进行 flush。最小值可配置 100ms。

## 数据删除与更新

每一个 segment 创建成功后都不会允许再被修改，因此删除和更新操作都无法修改原来的 segment。

对于删除操作，每一个提交点都会包含一个 `.del` 文件，文件中记录了哪一个 segment 的哪一个文档已经被删除了。被删除的文档依旧可以被搜索到，但是它将会在最终结果返回时被移除掉。

而当文档更新时，旧版本的文档将会被标记为删除，新版本的文档在新的 segment 中建立索引。也许新旧版本的文档都会本检索到，但是旧版本的文档会在最终结果返回时被移除。

## Segment 合并

通过每隔一秒的自动刷新机制会创建一个新的 segment，用不了多久就会有很多的 segment。segment 会消耗系统的文件句柄，内存，CPU时钟等。最重要的是，每一次请求都会依次检查所有的 segment。segment 越多，检索就会越慢。

ES 会有一个后台 merge 进程来合并 segment，该进程会选择一些小的 segments，然后 merge 到一个大的 segment 中，这个过程不会打断索引的创建和检索。

Merge 的过程中会把那些记录在`.del`文件中的被删除的文档真正的清除掉，因为被标记为删除的文档不会被拷贝到合并后的 segment 中。

**当一个 segment 大于可用内存的一半的时候，系统不会再对该 segment 做 merge 操作**，你可以想一想这是为什么呢？